{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzTLWowiuMjM"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68Yc3LKyrutT",
    "outputId": "9de4bd77-db72-4335-b013-675394e5e7e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thappana\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "print(\"Tensorflow Version\",tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGEE9VzuwP09"
   },
   "source": [
    "### About this file\n",
    "\n",
    "This is the sentiment140 dataset.\n",
    "It contains 1,600,000 tweets extracted using the twitter api . \n",
    "\n",
    "The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\n",
    "\n",
    "It contains the following 6 fields:\n",
    "\n",
    "1. `target`: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "2. `ids`: The id of the tweet ( 2087)\n",
    "\n",
    "3. `date`: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "4. `flag`: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "5. `user`: the user that tweeted (robotickilldozr)\n",
    "\n",
    "6. `text`: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFDHf8YotVtw"
   },
   "source": [
    "1. Read the .csv file and set it as a Dataframe called text_data. Check the head, info, and describe methods on the Dataframe(3 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lLe1jKoatxpf",
    "outputId": "b3c94664-3cbd-465e-eaac-f8b99be41729"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./training.1600000.processed.noemoticon.csv\"\n",
    "text_data = pd.read_csv(file_path, encoding = 'latin')\n",
    "text_data.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpOsBATHxX8Y",
    "outputId": "635ab2f3-e57d-4111-cabb-92f266f869ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text data:  (1599999, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of text data: \",text_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxzGl_9jxa2S",
    "outputId": "12e30937-e8fc-4118-932f-720c341ccdc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1599999 non-null  int64 \n",
      " 1   ids     1599999 non-null  int64 \n",
      " 2   date    1599999 non-null  object\n",
      " 3   flag    1599999 non-null  object\n",
      " 4   user    1599999 non-null  object\n",
      " 5   text    1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "text_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "fw8dLf1bxmKY",
    "outputId": "5a19c487-bc54-4b2b-f769-a8c425b82412"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.599999e+06</td>\n",
       "      <td>1.599999e+06</td>\n",
       "      <td>1599999</td>\n",
       "      <td>1599999</td>\n",
       "      <td>1599999</td>\n",
       "      <td>1599999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>774362</td>\n",
       "      <td>1</td>\n",
       "      <td>659775</td>\n",
       "      <td>1581465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jun 15 12:53:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>isPlayer Has Died! Sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1599999</td>\n",
       "      <td>549</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935757e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467811e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target           ids                          date      flag  \\\n",
       "count   1.599999e+06  1.599999e+06                       1599999   1599999   \n",
       "unique           NaN           NaN                        774362         1   \n",
       "top              NaN           NaN  Mon Jun 15 12:53:14 PDT 2009  NO_QUERY   \n",
       "freq             NaN           NaN                            20   1599999   \n",
       "mean    2.000001e+00  1.998818e+09                           NaN       NaN   \n",
       "std     2.000001e+00  1.935757e+08                           NaN       NaN   \n",
       "min     0.000000e+00  1.467811e+09                           NaN       NaN   \n",
       "25%     0.000000e+00  1.956916e+09                           NaN       NaN   \n",
       "50%     4.000000e+00  2.002102e+09                           NaN       NaN   \n",
       "75%     4.000000e+00  2.177059e+09                           NaN       NaN   \n",
       "max     4.000000e+00  2.329206e+09                           NaN       NaN   \n",
       "\n",
       "            user                       text  \n",
       "count    1599999                    1599999  \n",
       "unique    659775                    1581465  \n",
       "top     lost_dog  isPlayer Has Died! Sorry   \n",
       "freq         549                        210  \n",
       "mean         NaN                        NaN  \n",
       "std          NaN                        NaN  \n",
       "min          NaN                        NaN  \n",
       "25%          NaN                        NaN  \n",
       "50%          NaN                        NaN  \n",
       "75%          NaN                        NaN  \n",
       "max          NaN                        NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HBXuFdHsyQ6O",
    "outputId": "1b871bde-ad64-4b9e-b51b-8054b891d3b2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  is upset that he can't update his Facebook by ...\n",
       "1       0  @Kenichan I dived many times for the ball. Man...\n",
       "2       0    my whole body feels itchy and like its on fire \n",
       "3       0  @nationwideclass no, it's not behaving at all....\n",
       "4       0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping columns which are not required\n",
    "text_data = text_data.drop(['ids', 'date', 'flag', 'user'], axis=1)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeuHnqB0yolA",
    "outputId": "446d3714-c8c6-4204-db78-33cd43676e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    799999\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is a 50-50 ratio for positive and negative sentiments\n",
    "text_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "J6Hp3BlYy0IF",
    "outputId": "7ee4ef71-06bc-42f8-b7f1-ac47d9c3e3dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text\n",
       "0  Negative  is upset that he can't update his Facebook by ...\n",
       "1  Negative  @Kenichan I dived many times for the ball. Man...\n",
       "2  Negative    my whole body feels itchy and like its on fire \n",
       "3  Negative  @nationwideclass no, it's not behaving at all....\n",
       "4  Negative                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_emot = {0:\"Negative\", 4:\"Positive\"}\n",
    "def label_decoder(label):\n",
    "  return num_to_emot[label]\n",
    "\n",
    "text_data['target'] = text_data['target'].apply(lambda x: label_decoder(x))\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SGg-jbPtefT"
   },
   "source": [
    "2. Remove punctuations and stopwords from the text in “text” column (2Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m9fhqtc3tyc_"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pJ_57gM_zSpj"
   },
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in text.split():\n",
    "    if token not in stop_words:\n",
    "      if stem:\n",
    "        tokens.append(stemmer.stem(token))\n",
    "      else:\n",
    "        tokens.append(token)\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNesA1HEzWb5",
    "outputId": "2a5e37b4-17d6-456f-997d-22a3ce099a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.7 s ± 3.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# takes around 6 min to run\n",
    "text_data['text'] = text_data['text'].apply(lambda x: preprocess(x))\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L-5CKp5NASll"
   },
   "outputs": [],
   "source": [
    "text_data.to_csv('./preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_data=pd.read_csv('./preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAqmQMxbtkz2"
   },
   "source": [
    "3. Create two objects X and y. X will be the 'text’ column of Dataframe and y will be the 'target' column. Create a CountVectorizer object and split the data into training and testing sets. Train a MultinomialNB model and Display the confusion Matrix (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-X2R_AxctzJJ"
   },
   "outputs": [],
   "source": [
    "X = text_data['text']\n",
    "y = text_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FKIp9cP41ksr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          upset update facebook texting might cry result...\n",
       "1          dived many times ball managed save 50 rest go ...\n",
       "2                           whole body feels itchy like fire\n",
       "3                                           behaving mad see\n",
       "4                                                 whole crew\n",
       "                                 ...                        \n",
       "1599994                        woke school best feeling ever\n",
       "1599995             thewdb com cool hear old walt interviews\n",
       "1599996                      ready mojo makeover ask details\n",
       "1599997    happy 38th birthday boo alll time tupac amaru ...\n",
       "1599998    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, Length: 1599999, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Negative\n",
       "1          Negative\n",
       "2          Negative\n",
       "3          Negative\n",
       "4          Negative\n",
       "             ...   \n",
       "1599994    Positive\n",
       "1599995    Positive\n",
       "1599996    Positive\n",
       "1599997    Positive\n",
       "1599998    Positive\n",
       "Name: target, Length: 1599999, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sk79QG5fA83C"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ytpn8rHCZLL",
    "outputId": "b8b3cfce-6dc4-4417-f26e-3c340b755acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (1071999,)\n",
      "Shape of y_train (1071999,)\n",
      "Shape of X_test (528000,)\n",
      "Shape of y_test (528000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train\", X_train.shape)\n",
    "print(\"Shape of y_train\", y_train.shape)\n",
    "\n",
    "print(\"Shape of X_test\", X_test.shape)\n",
    "print(\"Shape of y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBnAfQQz0_F3",
    "outputId": "75d8147d-cfd6-4471-e986-fe38654e0cf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = MultinomialNB()\n",
    "\n",
    "text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IYI5LKZFxzQ",
    "outputId": "0a2aac47-59db-4f9d-9aee-5590361801bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 81.79334163665771 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer = 'word')\n",
    "\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "text_clf.fit(X_train_vect, y_train)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken to run: {(t2-t1)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Me3u0ueuGD3G",
    "outputId": "870c7304-69ae-4d23-f4c0-6afc36d35367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.80      0.78    263320\n",
      "    Positive       0.79      0.75      0.77    264680\n",
      "\n",
      "    accuracy                           0.77    528000\n",
      "   macro avg       0.78      0.78      0.77    528000\n",
      "weighted avg       0.78      0.77      0.77    528000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = text_clf.predict(X_test_vect)\n",
    "print(classification_report(y_test, preds, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG8izdkntn5j"
   },
   "source": [
    "4. Display the HMM POS tagging on the first 4 rows of ‘text’ (2 Marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbdFBf9gHHxF",
    "outputId": "66faae33-d251-4cb9-b0fe-bc4e3eff0497"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thappana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\thappana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ub6umwNztzzs",
    "outputId": "e7a88f91-0656-4f53-e71f-43826192cf71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hate', 'NN'), ('sleeping', 'VBG'), ('alone', 'RB'), ('california', 'NN'), ('king', 'VBG'), ('sized', 'VBN'), ('bed', 'NN'), ('wish', 'NN')]\n",
      "\n",
      "[('f1', 'NN'), ('fair', 'NN')]\n",
      "\n",
      "[('today', 'NN'), ('going', 'VBG'), ('prue', 'JJ'), ('100', 'CD'), ('hard', 'JJ'), ('work', 'NN'), ('msn', 'NN'), ('fb', 'NN'), ('twitter', 'NN')]\n",
      "\n",
      "[('geez', 'NN'), ('sucks', 'NNS')]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "270196     None\n",
       "730322     None\n",
       "1470779    None\n",
       "694452     None\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(df):\n",
    "    df = df.reset_index(drop = True) \n",
    "    return df.sample(frac = 1)\n",
    "\n",
    "text_data_sample = read_data(text_data)\n",
    "\n",
    "def get_pos_tag(text):\n",
    "    lower_case = text.lower()\n",
    "    tokens = nltk.word_tokenize(lower_case)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    print(tags)\n",
    "    print()\n",
    "    \n",
    "text_data_sample = read_data(text_data)\n",
    "text_data_sample.head(4)[\"text\"].apply(get_pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YROJbcHetvWt"
   },
   "source": [
    "5. Parse the first 4 rows of ‘text’ using Viterbi Parser [Use toy_pcfg1 and toy_pcfg2 to get the probabilistic context free grammars; use the PCFG suitable for each sentence] (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FJGHJfJUt0kQ"
   },
   "outputs": [],
   "source": [
    "def demo():\n",
    "    \"\"\"\n",
    "    A demonstration of the probabilistic parsers.  The user is\n",
    "    prompted to select which demo to run, and how many parses should\n",
    "    be found; and then each parser is run on the same demo, and a\n",
    "    summary of the results are displayed.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    from nltk import tokenize\n",
    "    from nltk.grammar import PCFG\n",
    "    from nltk.parse import ViterbiParser\n",
    "\n",
    "    toy_pcfg1 = PCFG.fromstring(\n",
    "        \"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    toy_pcfg2 = PCFG.fromstring(\n",
    "        \"\"\"\n",
    "    S    -> NP VP         [1.0]\n",
    "    VP   -> V NP          [.59]\n",
    "    VP   -> V             [.40]\n",
    "    VP   -> VP PP         [.01]\n",
    "    NP   -> Det N         [.41]\n",
    "    NP   -> Name          [.28]\n",
    "    NP   -> NP PP         [.31]\n",
    "    PP   -> P NP          [1.0]\n",
    "    V    -> 'saw'         [.21]\n",
    "    V    -> 'ate'         [.51]\n",
    "    V    -> 'ran'         [.28]\n",
    "    N    -> 'boy'         [.11]\n",
    "    N    -> 'cookie'      [.12]\n",
    "    N    -> 'table'       [.13]\n",
    "    N    -> 'telescope'   [.14]\n",
    "    N    -> 'hill'        [.5]\n",
    "    Name -> 'Jack'        [.52]\n",
    "    Name -> 'Bob'         [.48]\n",
    "    P    -> 'with'        [.61]\n",
    "    P    -> 'under'       [.39]\n",
    "    Det  -> 'the'         [.41]\n",
    "    Det  -> 'a'           [.31]\n",
    "    Det  -> 'my'          [.28]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Define two demos.  Each demo has a sentence and a grammar.\n",
    "    demos = [\n",
    "        (\"I saw the man with my telescope\", toy_pcfg1),\n",
    "        (\"the boy saw Jack with Bob under the table with a telescope\", toy_pcfg2),\n",
    "    ]\n",
    "\n",
    "    # Ask the user which demo they want to use.\n",
    "    print()\n",
    "    for i in range(len(demos)):\n",
    "        print(f\"{i + 1:>3}: {demos[i][0]}\")\n",
    "        print(\"     %r\" % demos[i][1])\n",
    "        print()\n",
    "    print(\"Which demo (%d-%d)? \" % (1, len(demos)), end=\" \")\n",
    "    try:\n",
    "        snum = int(sys.stdin.readline().strip()) - 1\n",
    "        sent, grammar = demos[snum]\n",
    "    except:\n",
    "        print(\"Bad sentence number\")\n",
    "        return\n",
    "\n",
    "    # Tokenize the sentence.\n",
    "    tokens = sent.split()\n",
    "\n",
    "    parser = ViterbiParser(grammar)\n",
    "    all_parses = {}\n",
    "\n",
    "    print(f\"\\nsent: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "    parser.trace(3)\n",
    "    t = time.time()\n",
    "    parses = parser.parse_all(tokens)\n",
    "    time = time.time() - t\n",
    "    average = (\n",
    "        reduce(lambda a, b: a + b.prob(), parses, 0) / len(parses) if parses else 0\n",
    "    )\n",
    "    num_parses = len(parses)\n",
    "    for p in parses:\n",
    "        all_parses[p.freeze()] = 1\n",
    "\n",
    "    # Print some summary statistics\n",
    "    print()\n",
    "    print(\"Time (secs)   # Parses   Average P(parse)\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"%11.4f%11d%19.14f\" % (time, num_parses, average))\n",
    "    parses = all_parses.keys()\n",
    "    if parses:\n",
    "        p = reduce(lambda a, b: a + b.prob(), parses, 0) / len(parses)\n",
    "    else:\n",
    "        p = 0\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"%11s%11d%19.14f\" % (\"n/a\", len(parses), p))\n",
    "\n",
    "    # Ask the user if we should draw the parses.\n",
    "    print()\n",
    "    print(\"Draw parses (y/n)? \", end=\" \")\n",
    "    if sys.stdin.readline().strip().lower().startswith(\"y\"):\n",
    "        from nltk.draw.tree import draw_trees\n",
    "\n",
    "        print(\"  please wait...\")\n",
    "        draw_trees(*parses)\n",
    "\n",
    "    # Ask the user if we should print the parses.\n",
    "    print()\n",
    "    print(\"Print parses (y/n)? \", end=\" \")\n",
    "    if sys.stdin.readline().strip().lower().startswith(\"y\"):\n",
    "        for parse in parses:\n",
    "            print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2Xw05Totjix",
    "outputId": "191b1ce9-6991-4b2a-917d-04f410324b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1: I saw the man with my telescope\n",
      "     <Grammar with 17 productions>\n",
      "\n",
      "  2: the boy saw Jack with Bob under the table with a telescope\n",
      "     <Grammar with 23 productions>\n",
      "\n",
      "Which demo (1-2)?  Bad sentence number\n"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdbWLlQmIPuI",
    "outputId": "0580e39c-8a5b-42fc-a64c-d7bca0d1dba7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\thappana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    }
   ],
   "source": [
    "# Downloading and importing the brown corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_dbItGpPzA3",
    "outputId": "6e86e137-495a-429a-e836-6cf7ddb5fdb9"
   },
   "outputs": [],
   "source": [
    "# Getting the tagged sentences\n",
    "sent_tag = brown.tagged_sents()\n",
    "mod_sent_tag = []\n",
    "for s in sent_tag:\n",
    "  s.insert(0,('##','##'))\n",
    "  s.append(('&&','&&'))\n",
    "  mod_sent_tag.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ooTfmeE3P5fY"
   },
   "outputs": [],
   "source": [
    "# Splitting the data for train and test\n",
    "split_num = int(len(mod_sent_tag)*0.9)\n",
    "train_data = mod_sent_tag[0:split_num]\n",
    "test_data = mod_sent_tag[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NsbiyJ98P-31"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary whose keys are tags and values contain words which were assigned the correspoding tag\n",
    "# ex:- 'TAG':{word1: count(word1,'TAG')}\n",
    "train_word_tag = {}\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      try:\n",
    "        train_word_tag[t][w]+=1\n",
    "      except:\n",
    "        train_word_tag[t][w]=1\n",
    "    except:\n",
    "      train_word_tag[t]={w:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LGW5FgpqQFKO"
   },
   "outputs": [],
   "source": [
    "# Calculating the emission probabilities using train_word_tag\n",
    "train_emission_prob={}\n",
    "for k in train_word_tag.keys():\n",
    "  train_emission_prob[k]={}\n",
    "  count = sum(train_word_tag[k].values())\n",
    "  for k2 in train_word_tag[k].keys():\n",
    "    train_emission_prob[k][k2]=train_word_tag[k][k2]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "KAfss6owQIWP"
   },
   "outputs": [],
   "source": [
    "# Estimating the bigram of tags to be used for transition probability\n",
    "bigram_tag_data = {}\n",
    "for s in train_data:\n",
    "  bi=list(nltk.bigrams(s))\n",
    "  for b1,b2 in bi:\n",
    "    try:\n",
    "      try:\n",
    "        bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]][b2[1]]=1\n",
    "    except:\n",
    "      bigram_tag_data[b1[1]]={b2[1]:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "N8fUqWP9QPzs"
   },
   "outputs": [],
   "source": [
    "# Calculating the probabilities of tag bigrams for transition probability  \n",
    "bigram_tag_prob={}\n",
    "for k in bigram_tag_data.keys():\n",
    "  bigram_tag_prob[k]={}\n",
    "  count=sum(bigram_tag_data[k].values())\n",
    "  for k2 in bigram_tag_data[k].keys():\n",
    "    bigram_tag_prob[k][k2]=bigram_tag_data[k][k2]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0r-SiBRCQTCF"
   },
   "outputs": [],
   "source": [
    "# Calculating the possible tags for each word\n",
    "# Note: Here we have used the whole data(Train+Test)\n",
    "# Reason: There may be some words which are not present in train data but are present in test data \n",
    "tags_of_tokens = {}\n",
    "count=0\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n",
    "        \n",
    "for s in test_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "z7AYl7DyQZQl"
   },
   "outputs": [],
   "source": [
    "# Dividing the test data into test words and test tags\n",
    "test_words=[]\n",
    "test_tags=[]\n",
    "for s in test_data:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (w,t) in s:\n",
    "    temp_word.append(w.lower())\n",
    "    temp_tag.append(t)\n",
    "  test_words.append(temp_word)\n",
    "  test_tags.append(temp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "T2vI3pvYQdPw"
   },
   "outputs": [],
   "source": [
    "#Executing the Viterbi Algorithm\n",
    "predicted_tags = []                #intializing the predicted tags\n",
    "for x in range(len(test_words)):   # for each tokenized sentence in the test data\n",
    "  s = test_words[x]\n",
    "  #storing_values is a dictionary which stores the required values\n",
    "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
    "  storing_values = {}              \n",
    "  for q in range(len(s)):\n",
    "    step = s[q]\n",
    "    #for the starting word of the sentence\n",
    "    if q == 1:                \n",
    "      storing_values[q] = {}\n",
    "      tags = tags_of_tokens[step]\n",
    "      for t in tags:\n",
    "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "        try:\n",
    "          storing_values[q][t] = ['##',bigram_tag_prob['##'][t]*train_emission_prob[t][step]]\n",
    "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "        except:\n",
    "          storing_values[q][t] = ['##',0.0001]#*train_emission_prob[t][step]]\n",
    "    \n",
    "    #if the word is not at the start of the sentence\n",
    "    if q>1:\n",
    "      storing_values[q] = {}\n",
    "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "      current_states  = tags_of_tokens[step]               # loading the current states\n",
    "      #calculation of the best previous state for each current state and then storing\n",
    "      #it in storing_values\n",
    "      for t in current_states:                             \n",
    "        temp = []\n",
    "        for pt in previous_states:                         \n",
    "          try:\n",
    "            temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step])\n",
    "          except:\n",
    "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "        max_temp_index = temp.index(max(temp))\n",
    "        best_pt = previous_states[max_temp_index]\n",
    "        storing_values[q][t]=[best_pt,max(temp)]\n",
    "\n",
    "  #Backtracing to extract the best possible tags for the sentence\n",
    "  pred_tags = []\n",
    "  total_steps_num = storing_values.keys()\n",
    "  last_step_num = max(total_steps_num)\n",
    "  for bs in range(len(total_steps_num)):\n",
    "    step_num = last_step_num - bs\n",
    "    if step_num == last_step_num:\n",
    "      pred_tags.append('&&')\n",
    "      pred_tags.append(storing_values[step_num]['&&'][0])\n",
    "    if step_num<last_step_num and step_num>0:\n",
    "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0])\n",
    "  predicted_tags.append(list(reversed(pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1wkXt14Rb3V",
    "outputId": "5ded7196-93ad-4492-8b4f-c054d793743b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data is:  0.9214105322233654\n",
      "Loss on the test data is:  0.07858946777663456\n"
     ]
    }
   ],
   "source": [
    "#Calculating the accuracy based on tagging each word in the test data.\n",
    "right = 0 \n",
    "wrong = 0\n",
    "for i in range(len(test_tags)):\n",
    "  gt = test_tags[i]\n",
    "  pred = predicted_tags[i]\n",
    "  for h in range(len(gt)):\n",
    "    if gt[h] == pred[h]:\n",
    "      right = right+1\n",
    "    else:\n",
    "      wrong = wrong +1 \n",
    "\n",
    "print('Accuracy on the test data is: ',right/(right+wrong))\n",
    "print('Loss on the test data is: ',wrong/(right+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "OMMxYLiyRowN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thappana\\AppData\\Local\\Temp/ipykernel_116892/1730537761.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sentences = np.array(brown.tagged_sents())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  ['taboooo', 'fun']\n",
      "['NN']\n",
      "--- 20.37156891822815 seconds ---\n",
      "taboooo,\n",
      "fun,\n",
      "[('taboooo', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import time\n",
    "\n",
    "stime = time.time()\n",
    "sentences = np.array(brown.tagged_sents())\n",
    "words = brown.tagged_words()\n",
    "tokens,taged = zip(*words)\n",
    "\n",
    "#\n",
    "# firstdict = {}\n",
    "# firstSum = len(sentences)\n",
    "# for i in sentences:\n",
    "#     x,y = i[0]\n",
    "#     if y not in firstdict.keys():\n",
    "#         firstdict[y] = 1\n",
    "#     else:\n",
    "#         firstdict[y] += 1\n",
    "#\n",
    "# for i in firstdict.keys():\n",
    "#     firstdict[i] = firstdict[i]/firstSum\n",
    "\n",
    "# total word count\n",
    "total = len(words)\n",
    "\n",
    "# preping corpus data\n",
    "wordcount = Counter(tokens)\n",
    "tokenTags = defaultdict(Counter)\n",
    "for token, tag in words:\n",
    "    tokenTags[token][tag] += 1\n",
    "\n",
    "tagcount = Counter(taged)\n",
    "for i in tagcount.keys():\n",
    "    tagcount[i] = tagcount[i]/total\n",
    "\n",
    "bgram = nltk.ngrams(taged,2)\n",
    "tagtags = defaultdict(Counter)\n",
    "for tag1, tag2 in bgram:\n",
    "    tagtags[tag1][tag2] += 1\n",
    "\n",
    "\n",
    "#viterbi implementation\n",
    "trans = {}\n",
    "StateProbs = {}\n",
    "def viterbi(prior,transition,num):\n",
    "    a = []\n",
    "    trans[num] = []\n",
    "    StateProbs[num+1] = []\n",
    "    emmision = tokenTags[test[num]]\n",
    "    wn =wordcount[test[num]]\n",
    "    p = {}\n",
    "    for ik,ii in emmision.items():\n",
    "        #hold probs\n",
    "        min = 100000\n",
    "        for jk,ji in prior.items():\n",
    "            if transition[jk][ik] != 0:\n",
    "                if num==0:\n",
    "                    prob = log((ii/wn),2)  + (log((transition[jk][ik]/(total-1)), 2))\n",
    "                else:\n",
    "                    prob = ji + log((ii/wn), 2) + (log((transition[jk][ik]/(total-1)), 2))\n",
    "                trans[num].append([(jk,ik),prob])\n",
    "                if min > prob:\n",
    "                    min = prob\n",
    "        p[ik] = min\n",
    "        StateProbs[num+1].append([ik,min])\n",
    "    return p\n",
    "\n",
    "#sentence and test\n",
    "#test = ['Time','flies','like','an','arrow','.']\n",
    "test = text_data.sample()[\"text\"].values.tolist()[0].split(' ')\n",
    "print(\"test: \", test)\n",
    "prev = viterbi(tagcount,tagtags,0)\n",
    "for i in range(1,len(test)):\n",
    "    prev = viterbi(prev,tagtags,i)\n",
    "\n",
    "del trans[0]\n",
    "\n",
    "prevP = 0\n",
    "prev = ''\n",
    "order = []\n",
    "\n",
    "#backpropogation\n",
    "for i in range(len(test)-1,-1,-1):\n",
    "    if i == len(test)-1:\n",
    "        min = 100000000\n",
    "        for j in StateProbs[i+1]:\n",
    "            if min > j[1]:\n",
    "                prev = j[0]\n",
    "                min = j[1]\n",
    "                prevP = j[1]\n",
    "        order.append(prev)\n",
    "    else:\n",
    "        for g in trans[i+1]:\n",
    "            if prevP == g[1]:\n",
    "                x,y = g[0]\n",
    "                prev = x\n",
    "                order.append(prev)\n",
    "        for k in StateProbs[i+1]:\n",
    "            if k[0] == prev:\n",
    "                prevP = k[1]\n",
    "\n",
    "#solution\n",
    "sol = []\n",
    "for i in reversed(order):\n",
    "    sol.append(i)\n",
    "print(sol)\n",
    "print(\"--- %s seconds ---\" % (time.time() - stime))\n",
    "for t in test:\n",
    "    print(t,end=',')\n",
    "    print()\n",
    "list_zip = zip(test, reversed(order)) \n",
    "print(list(list_zip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  270196    [hate, sleeping, alone, california, king, size...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test = text_data_sample.head(1)[\"text\"].str.split(\" \")\n",
    "print(\"test: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sprint', 'store', 'see', 'figure', 'camera', 'work']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = text_data.sample()[\"text\"].values.tolist()[0].split(' ')\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment Set 56.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
